{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "## Theano\n",
    "\n",
    "  * Python library that provides efficient (low-level) tools for working with Neural Networks\n",
    "  * In particular:\n",
    "      * Automatic Differentiation (AD)\n",
    "      * Compiled computation graphs\n",
    "      * GPU accelerated computation\n",
    "\n",
    "## Keras\n",
    "\n",
    "   * High level library for specifying and training neural networks\n",
    "   * Can use `Theano` or `TensorFlow` as backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "\n",
    "  * 60,000 handwritten digits\n",
    "  * As 28x28 pixel images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "  * Replace magic constants with variables\n",
    "  * Differentiate between images_train and X_train\n",
    "  * make random examples interactive widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "nb_classes = 10\n",
    "nb_epoch = 1\n",
    "nb_params = 512   # rename to nb_hidden\n",
    "batch_size = 128\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_10_by_10_images(images, figsize=None):\n",
    "    \"\"\" Plot 100 MNIST images in a 10 by 10 table. Note that we crop\n",
    "    the images so that they appear reasonably close together.  The\n",
    "    image is post-processed to give the appearance of being continued.\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    #images = [image[3:25, 3:25] for image in images]\n",
    "    #image = np.concatenate(images, axis=1)\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            ax = fig.add_subplot(10, 10, 10*y+x+1)\n",
    "            ax.matshow(images[10*y+x], cmap = matplotlib.cm.binary)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mnist_digit(image, figsize=None):\n",
    "    \"\"\" Plot a single MNIST image.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    if figsize:\n",
    "        ax.set_figsize(*figsize)\n",
    "    ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_1_by_2_images(image, reconstruction, figsize=None):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.matshow(reconstruction, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(len(X_train))\n",
    "plot_mnist_digit(X_train[i])\n",
    "print(i, ':', y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_features(X):\n",
    "    return X.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "def to_images(X):\n",
    "    return (X*255.0).astype('uint8').reshape(-1, 28, 28)\n",
    "\n",
    "print((X_train[0]-(to_images(to_features(X_train[0])))).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "X_train = to_features(X_train)\n",
    "X_test = to_features(X_test)\n",
    "print(X_train.shape, 'train samples')\n",
    "print(X_test.shape, 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The labels need to be transformed into class indicators\n",
    "from keras.utils import np_utils\n",
    "y_train_cat = np_utils.to_categorical(y_train, nb_classes=nb_classes)\n",
    "y_test_cat = np_utils.to_categorical(y_test, nb_classes=nb_classes)\n",
    "print(y_train_cat.shape, 'train labels')\n",
    "print(y_test_cat.shape, 'test labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(output_dim=nb_params, input_dim=784, init='uniform'))\n",
    "mlp.add(Activation('sigmoid'))\n",
    "mlp.add(Dense(output_dim=nb_classes, input_dim=nb_params, init='uniform'))\n",
    "mlp.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp.compile(loss='categorical_crossentropy', optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X_train, y_train_cat, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "        verbose=1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.evaluate(X_test, y_test_cat, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = np.random.randint(len(X_test))\n",
    "plot_mnist_digit(to_images(X_test)[j])\n",
    "prediction = mlp.predict_classes(X_test[j:j+1], verbose=False)[0]\n",
    "print(j, ':', '\\tpredict:', prediction, '\\tactual:', y_test[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deeper MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "mlp2 = Sequential()\n",
    "mlp2.add(Dense(output_dim=nb_params/2, input_dim=784, init='uniform'))\n",
    "mlp2.add(Activation('sigmoid'))\n",
    "mlp2.add(Dense(output_dim=nb_params/2, input_dim=nb_params/2, init='uniform'))\n",
    "mlp2.add(Activation('sigmoid'))\n",
    "mlp2.add(Dense(output_dim=nb_classes, input_dim=nb_params/2, init='uniform'))\n",
    "mlp2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp2.compile(loss='categorical_crossentropy', optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp2.fit(X_train, y_train_cat, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp2.evaluate(X_test, y_test_cat, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Manual Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "mae = Sequential()\n",
    "nb_layers = 1\n",
    "nb_params = 1000\n",
    "encoder = []\n",
    "decoder = []\n",
    "for i in range(nb_layers):\n",
    "    if i>0:\n",
    "        encoder.append(Dropout(0.4))\n",
    "    encoder.append(Dense(output_dim=nb_params/nb_layers,\n",
    "                         input_dim=784 if i==0 else nb_params/nb_layers,\n",
    "                         init='glorot_uniform'))\n",
    "    encoder.append(Activation('sigmoid'))\n",
    "    \n",
    "    # Note that these are in reverse order\n",
    "    decoder.append(Activation('sigmoid'))\n",
    "    decoder.append(Dense(output_dim=784 if i==0 else nb_params/nb_layers,\n",
    "                         input_dim=nb_params/nb_layers,\n",
    "                         init='glorot_uniform'))\n",
    "    #decoder.append(Dropout(0.2))\n",
    "\n",
    "for layer in encoder:\n",
    "    mae.add(layer)\n",
    "for layer in reversed(decoder):\n",
    "    mae.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#sgd = SGD(lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mae.compile(loss='mse', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mae.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = np.random.randint(len(X_train))\n",
    "X_plot = X_train[j:j+1]\n",
    "prediction = mae.predict(X_plot, verbose=False)\n",
    "plot_1_by_2_images(to_images(X_plot)[0], to_images(prediction)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wgts = mae.get_weights()\n",
    "print(len(wgts))\n",
    "for i, w in enumerate(wgts):\n",
    "    print(i, w.shape)\n",
    "w = wgts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = np.random.randint(w.shape[1])\n",
    "X_plot = w[:,j:j+1]\n",
    "plot_mnist_digit(to_images(X_plot.T)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "class StackedAutoencoder(object):\n",
    "    \n",
    "    def __init__(self, layers,\n",
    "                 activation='sigmoid', init='uniform',\n",
    "                 dropout=0.2, optimizer='SGD'):\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        self.init = init\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.build()\n",
    "        self.compile()\n",
    "        \n",
    "    def build(self):\n",
    "        autoencoder = Sequential()\n",
    "        encoder = []\n",
    "        decoder = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            if i>0:\n",
    "                encoder.append(Dropout(self.dropout))\n",
    "            encoder.append(Dense(output_dim=self.layers[i+1],\n",
    "                                 input_dim=self.layers[i],\n",
    "                                 init=self.init))\n",
    "            encoder.append(Activation(self.activation))\n",
    "\n",
    "            # Note that the decoder layers are in reverse order\n",
    "            decoder.append(Activation(self.activation))\n",
    "            decoder.append(Dense(output_dim=self.layers[i],\n",
    "                                 input_dim=self.layers[i+1], \n",
    "                                 init=self.init))\n",
    "        for layer in encoder:\n",
    "            autoencoder.add(layer)\n",
    "        for layer in reversed(decoder):\n",
    "            autoencoder.add(layer)\n",
    "            \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "    def compile(self):\n",
    "        return self.autoencoder.compile(loss='mse', optimizer=self.optimizer)\n",
    "    \n",
    "    def fit(self, X_train, Y_train, batch_size, nb_epoch, verbose=1):\n",
    "        return self.autoencoder.fit(X_train, Y_train,\n",
    "                                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                                    verbose=verbose)\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test, show_accuracy=False):\n",
    "        return self.autoencoder.evaluate(X_test, Y_test, show_accuracy=show_accuracy)\n",
    "    \n",
    "    def predict(self, X, verbose=False):\n",
    "        return self.autoencoder.predict(X, verbose=verbose)\n",
    "\n",
    "    def _get_paths(self, name):\n",
    "        model_path = \"models/{}_model.yaml\".format(name)\n",
    "        weights_path = \"models/{}_weights.hdf5\".format(name)\n",
    "        return model_path, weights_path\n",
    "\n",
    "    def save(self, name='autoencoder'):\n",
    "        model_path, weights_path = self._get_paths(name)\n",
    "        open(model_path, 'w').write(self.autoencoder.to_yaml())\n",
    "        self.autoencoder.save_weights(weights_path, overwrite=True)\n",
    "    \n",
    "    def load(self, name='autoencoder'):\n",
    "        model_path, weights_path = self._get_paths(name)\n",
    "        self.autoencoder = keras.models.model_from_yaml(open(model_path))\n",
    "        self.autoencoder.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "sae = StackedAutoencoder(layers=[784, nb_params],\n",
    "                         activation='sigmoid', init='uniform',\n",
    "                         dropout=0.2, optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sae.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = np.random.randint(len(X_train))\n",
    "X_plot = X_train[j:j+1]\n",
    "prediction = sae.predict(X_plot, verbose=False)\n",
    "plot_1_by_2_images(to_images(X_plot)[0], to_images(prediction)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sae.evaluate(X_test, X_test, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wgts = sae.autoencoder.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, w in enumerate(wgts):\n",
    "    print(i, w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions that I have\n",
    "\n",
    "  * How often are the Dropout neurons reset? Once an epoch or more often?\n",
    "  * Is Dropout removed during the predict() call?\n",
    "  * Should inputs be centered on zero for Dropout to be appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
