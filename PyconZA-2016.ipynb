{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Deep Learning in Python #\n",
    "\n",
    "  * Get the code: [github.com/snth/ctdeep](github.com/snth/ctdeep)\n",
    "\n",
    "## Tobias Brandt ##\n",
    "\n",
    "<img src=\"img/argon_logo.png\" align=left width=200>\n",
    "\n",
    "<!-- <img src=\"http://www.argonassetmanagement.co.za/css/img/logo.png\" align=left width=200> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## About Me\n",
    "\n",
    "  * ex-physicist, quant, pythonista\n",
    "  * github.com/snth\n",
    "  * tobias@argonasset.co.za\n",
    "  * Member of [Cape Town Deep Learning Meetup](http://www.meetup.com/Cape-Town-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tutorial Outline\n",
    "\n",
    " 1. Deep Learning in Python is simple and powerful!\n",
    " 2. An introduction to (Artificial) Neural Networks\n",
    " 3. An introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "  * Python 3.4 (or legacy Python 2.7)\n",
    "  * Keras >= 1.0.0\n",
    "  * Theano or Tensorflow\n",
    "  * `git clone https://github.com/snth/ctdeep.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning in Python is simple\n",
    "\n",
    "![xkcd antigravity](img/xkcd-353.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %load ..\\keras\\examples\\mnist_cnn.py\n",
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "(images_train, labels_train), (images_test, labels_test) = (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Compiling the model ...\")\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "#model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "#          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    weights_path = \"models/mnist_cnn_{}_weights.h5\".format(epoch)\n",
    "    if os.path.exists(weights_path):\n",
    "        print(\"Loading precomputed weights for epoch {} ...\".format(epoch))\n",
    "        model.load_weights(weights_path)\n",
    "        print('Evaluating the model on the test set ...')\n",
    "        score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "        print('Test score:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "    else:\n",
    "        print(\"Fitting the model for epoch {} ...\".format(epoch))\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n",
    "                  validation_data=(X_test, Y_test), verbose=1)\n",
    "        model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Evaluating the model on the test set ...')\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So building and training Neural Networks in Python in simple!\n",
    "\n",
    "But it is also powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neural Style Transfer: [github.com/titu1994/Neural-Style-Transfer](https://github.com/titu1994/Neural-Style-Transfor)\n",
    "\n",
    "<font size=20>\n",
    "<table border=\"0\"><tr>\n",
    "<td><img src=\"img/golden_gate.jpg\" width=250></td>\n",
    "<td>+</td>\n",
    "<td><img src=\"img/starry_night.jpg\" width=250></td>\n",
    "<td>=</td>\n",
    "<td><img src=\"img/golden_gate_iteration_20.png\" width=250></td>\n",
    "</tr></table>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "<!-- \n",
    "\n",
    "[Yann LeCun Slides](https://drive.google.com/folderview?id=0BxKBnD5y2M8NclFWSXNxa0JlZTg&usp=drive_web) ([local](pdf/000c-yann-lecun-lecon-inaugurale-college-de-france-20160204.pdf))\n",
    "\n",
    "![Intelligent Systems](img/LeCun_flight.png) -->\n",
    "\n",
    "<img src='img/LeCun_flight.png' align='middle' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A neuron looks something like this\n",
    "\n",
    "![A neuron](img/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Symbolically we can represent the key parts we want to model as\n",
    "\n",
    "![A simplified neuron](img/simplified_neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to build an artifical \"brain\" we need to connect together many neurons in a \"neural network\"\n",
    "\n",
    "![Neural Network](img/neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can model the response of each neuron with various *activation functions*\n",
    "\n",
    "![Activations](img/activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training a Neural Network\n",
    "\n",
    "<!--\n",
    "\n",
    "![Perceptron Model](img/perceptron_node.png) -->\n",
    "\n",
    "<img src='img/perceptron_node.png' align='middle' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically the activation of each neuron can be represented by\n",
    "\n",
    "![Neuron Activation](img/matrix_mult.png)\n",
    "\n",
    "where $W$ and $b$ are the **weights** and **bias** respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Loss Function\n",
    "\n",
    "<!--\n",
    "![Loss Function](img/loss_function.png)\n",
    "-->\n",
    "<img src='img/loss_function.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Table Mountain](img/table_mountain_with_names.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Table Mountain Contours](img/table_mountain_model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in Python\n",
    "\n",
    "## Keras\n",
    "\n",
    "   * High level library for specifying and training neural networks\n",
    "   * Can use **`Theano`** or **`TensorFlow`** as backend\n",
    "\n",
    "### Keras makes Neural Networks awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Theano\n",
    "\n",
    "  * Python library that provides efficient (low-level) tools for working with Neural Networks\n",
    "  * In particular:\n",
    "      * Automatic Differentiation (AD)\n",
    "      * Compiled computation graphs\n",
    "      * GPU accelerated computation\n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "  * Deep Learning framework by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The MNIST Dataset\n",
    "\n",
    "  * 70,000 handwritten digits\n",
    "      * 60,000 for training\n",
    "      * 10,000 for testing\n",
    "  * As 28x28 pixel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "from ipywidgets import interact, interactive, widgets\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "#(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "print(\"Data shapes:\")\n",
    "print('images',images_train.shape)\n",
    "print('labels', labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and then visualise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mnist_digit(image, figsize=None):\n",
    "    \"\"\" Plot a single MNIST image.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    if figsize:\n",
    "        ax.set_figsize(*figsize)\n",
    "    ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_1_by_2_images(image, reconstruction, figsize=None):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.matshow(reconstruction, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_10_by_10_images(images, figsize=None):\n",
    "    \"\"\" Plot 100 MNIST images in a 10 by 10 table. Note that we crop\n",
    "    the images so that they appear reasonably close together.  The\n",
    "    image is post-processed to give the appearance of being continued.\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    #images = [image[3:25, 3:25] for image in images]\n",
    "    #image = np.concatenate(images, axis=1)\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            ax = fig.add_subplot(10, 10, 10*y+x+1)\n",
    "            ax.matshow(images[10*y+x], cmap = matplotlib.cm.binary)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_10_by_20_images(left, right, figsize=None):\n",
    "    \"\"\" Plot 100 MNIST images next to their reconstructions\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            ax = fig.add_subplot(10, 21, 21*y+x+1)\n",
    "            ax.matshow(left[10*y+x], cmap = matplotlib.cm.binary)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "            ax = fig.add_subplot(10, 21, 21*y+11+x+1)\n",
    "            ax.matshow(right[10*y+x], cmap = matplotlib.cm.binary)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_10_by_10_images(images_train, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_image(i):\n",
    "    plot_mnist_digit(images_train[i])\n",
    "    print('label:', labels_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(draw_image, i=(0, len(images_train)-1))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Transform \"images\" to \"features\" ...\n",
    "\n",
    "Most machine learning algorithms expect a flat array of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def to_features(X):\n",
    "    return X.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "def to_images(X):\n",
    "    return (X*255.0).astype('uint8').reshape(-1, 28, 28)\n",
    "\n",
    "print('data shape:', images_train.shape, images_train.dtype)\n",
    "print('features shape', to_features(images_train).shape, to_features(images_train).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split the data into a \"training\" and \"test\" set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "X_train = to_features(images_train)\n",
    "X_test = to_features(images_test)\n",
    "print(X_train.shape, 'training samples')\n",
    "print(X_test.shape, 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transform the labels to a \"one-hot\" encoding ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The labels need to be transformed into class indicators\n",
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(labels_train, nb_classes=10)\n",
    "y_test = np_utils.to_categorical(labels_test, nb_classes=10)\n",
    "print('labels_train:', labels_train.shape, labels_train.dtype)\n",
    "print('y_train:', y_test.shape, y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, let's inspect the first 2 labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('labels_train[:2]:\\n', labels_train[:2][:, np.newaxis])\n",
    "print('y_train[:2]\\n', y_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Multi-Layer Perceptron (MLP)\n",
    "\n",
    "The simplest kind of Artificial Neural Network is as Multi-Layer Perceptron (MLP) with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network Architecture Parameters\n",
    "nb_input = 784\n",
    "nb_hidden = 512\n",
    "nb_output = 10\n",
    "# Training Parameters\n",
    "nb_epoch = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we define the \"architecture\" of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "mlp = Sequential()\n",
    "\n",
    "mlp.add(Dense(output_dim=nb_hidden, input_dim=nb_input, init='uniform'))\n",
    "mlp.add(Activation('sigmoid'))\n",
    "\n",
    "mlp.add(Dense(output_dim=nb_output, input_dim=nb_hidden, init='uniform'))\n",
    "mlp.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "then we compile it. This takes the symbolic computational graph of the model and compiles it an efficient implementation which can then be used to train and evaluate the model. \n",
    "\n",
    "Note that we have to specify what loss/objective function we want to use as well which optimisation algorithm to use. **SGD** stands for Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mlp.compile(loss='categorical_crossentropy', optimizer='SGD',\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next we train the model on our training data. Watch the loss, which is the objective function which we are minimising, and the estimated accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mlp.fit(X_train, y_train, \n",
    "        batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the model is trained, we can evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mlp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#plot_10_by_10_images(images_test, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_mlp_prediction(j):\n",
    "    plot_mnist_digit(to_images(X_test)[j])\n",
    "    prediction = mlp.predict_classes(X_test[j:j+1], verbose=False)[0]\n",
    "    print('predict:', prediction, '\\tactual:', labels_test[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(draw_mlp_prediction, j=(0, len(X_test)-1))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "Why do we want Deep Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "\n",
    "    The theorem thus states that simple neural networks can represent \n",
    "    a wide variety of interesting functions when given appropriate parameters;\n",
    "    however, it does not touch upon the algorithmic learnability of those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Power of combinations\n",
    "\n",
    "[On the (Small) Number of Atoms in the Universe](http://norvig.com/atoms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### On the number of Go positions ###\n",
    "\n",
    "While <a href=\"https://www.theguardian.com/technology/2016/mar/09/google-deepmind-alphago-ai-defeats-human-lee-sedol-first-game-go-contest\">discussing</a> the complexity of the game of Go, <a href=\"https://en.wikipedia.org/wiki/Demis_Hassabis\">Demis Hassabis</a>  said:\n",
    "\n",
    "<blockquote>\n",
    "<i>There are more possible Go positions than there are atoms in the universe.</i>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A Go board has 19 &times; 19 points, each of which can be empty or occupied by black or white, so there are 3<sup>(19 &times; 19)</sup> <tt>&cong;</tt> 10<sup>172</sup> possible board positions, but  \"only\" about  10<sup>170</sup> of those positions are legal.\n",
    "\n",
    "<p>The crucial idea is, that as a number of <i>physical things</i>, 10<sup>80</sup> is a really big number. But as a number of <i>combinations</i> of things, 10<sup>80</sup> is a rather small number. It doesn't take a universe of stuff to get up to 10<sup>80</sup> combinations; we can get there with, for example, a passphrase field that is 40 characters long:\n",
    "\n",
    "<blockquote id=\"passphrase\">\n",
    "<tt>a correct horse battery staple troubador</tt>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### On the number of digital pictures ###\n",
    "\n",
    "There is an art project to display every possible picture. Surely that would take a long time, because there must be many possible pictures. But how many?\n",
    "\n",
    "We will assume the color model known as True Color, in which each pixel can be one of 2^24 ≅ 17 million distinct colors. The digital camera shown below left has 12 million pixels, and we'll also consider much smaller pictures: the array below middle, with 300 pixels, and the array below right with just 12 pixels; shown are some of the possible pictures:\n",
    "\n",
    "<img src=\"img/norvig_atoms.png\" align=\"center\">\n",
    "\n",
    "**Quiz: Which of these produces a number of pictures similar to the number of atoms in the universe?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Answer: An array of n pixels produces (17 million)^n different pictures. (17 million)^12 ≅ 10^86, so the tiny 12-pixel array produces a million times more pictures than the number of atoms in the universe!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "How about the 300 pixel array? It can produce 10^2167 pictures. You may think the number of atoms in the universe is big, but that's just peanuts to the number of pictures in a 300-pixel array. And 12M pixels? 10^86696638 pictures. Fuggedaboutit!\n",
    "\n",
    "So the number of possible pictures is really, really, really big. And the number of atoms in the universe is looking relatively small, at least as a number of combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### ==> The Curse of Dimensionality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Hierarchies\n",
    "\n",
    "<!--\n",
    "![Feature Hierarchy](img/feature_hierarchy.png)\n",
    "-->\n",
    "\n",
    "<img src=\"img/feature_hierarchy.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Deeper MLP\n",
    "\n",
    "Next we build a two-layer MLP with the same number of hidden nodes, half in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "nb_layers = 2\n",
    "mlp2 = Sequential()\n",
    "# add hidden layers\n",
    "for i in range(nb_layers):\n",
    "    mlp2.add(Dense(output_dim=nb_hidden//nb_layers, input_dim=nb_input if i==0 else nb_hidden//nb_layers, init='uniform'))\n",
    "    mlp2.add(Activation('sigmoid'))\n",
    "# add output layer\n",
    "mlp2.add(Dense(output_dim=nb_output, input_dim=nb_hidden//nb_layers, init='uniform'))\n",
    "mlp2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mlp2.compile(loss='categorical_crossentropy', optimizer='SGD',\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mlp2.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Did you notice anything about the accuracy? Let's train it some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mlp2.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mlp2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Hinton 2006](https://www.cs.toronto.edu/~hinton/science.pdf) ([local](pdf/Hinton2006-science.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"pdf/Hinton2006-science.pdf\" width=800 height=400></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "print('nb_input =', nb_input)\n",
    "print('nb_hidden =', nb_hidden)\n",
    "ae = Sequential()\n",
    "# encoder\n",
    "ae.add(Dense(nb_hidden, input_dim=nb_input, init='uniform'))\n",
    "ae.add(Activation('sigmoid'))\n",
    "# decoder\n",
    "ae.add(Dense(nb_input, input_dim=nb_hidden, init='uniform'))\n",
    "ae.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ae.compile(loss='mse', optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb_epoch = 1\n",
    "ae.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_10_by_20_images(images_test, to_images(ae.predict(X_test)),\n",
    "                     figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ae.compile(loss='mse', optimizer=sgd)\n",
    "nb_epoch = 1\n",
    "ae.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_10_by_20_images(images_test, to_images(ae.predict(X_test)),\n",
    "                     figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_ae_prediction(j):\n",
    "    X_plot = X_test[j:j+1]\n",
    "    prediction = ae.predict(X_plot, verbose=False)\n",
    "    plot_1_by_2_images(to_images(X_plot)[0], to_images(prediction)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(draw_ae_prediction, j=(0, len(X_test)-1))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A better Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "def make_autoencoder(nb_input=nb_input, nb_hidden=nb_hidden,\n",
    "                     activation='sigmoid', init='uniform'):\n",
    "    ae = Sequential()\n",
    "    # encoder\n",
    "    ae.add(Dense(nb_hidden, input_dim=nb_input, init=init))\n",
    "    ae.add(Activation(activation))\n",
    "    # decoder\n",
    "    ae.add(Dense(nb_input, input_dim=nb_hidden, init=init))\n",
    "    ae.add(Activation(activation))\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb_epoch = 1\n",
    "ae2 = make_autoencoder(activation='sigmoid', init='glorot_uniform')\n",
    "ae2.compile(loss='mse', optimizer='adam')\n",
    "ae2.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_10_by_20_images(images_test, to_images(ae2.predict(X_test)), figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_ae2_prediction(j):\n",
    "    X_plot = X_test[j:j+1]\n",
    "    prediction = ae2.predict(X_plot, verbose=False)\n",
    "    plot_1_by_2_images(to_images(X_plot)[0], to_images(prediction)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(draw_ae2_prediction, j=(0, len(X_test)-1))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "class StackedAutoencoder(object):\n",
    "    \n",
    "    def __init__(self, layers, mode='autoencoder',\n",
    "                 activation='sigmoid', init='uniform', final_activation='softmax',\n",
    "                 dropout=0.2, optimizer='SGD', metrics=None):\n",
    "        self.layers = layers\n",
    "        self.mode = mode\n",
    "        self.activation = activation\n",
    "        self.final_activation = final_activation\n",
    "        self.init = init\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        self._model = None\n",
    "        \n",
    "        self.build()\n",
    "        self.compile()\n",
    "    \n",
    "    def _add_layer(self, model, i, is_encoder):\n",
    "        if is_encoder:\n",
    "            input_dim, output_dim = self.layers[i], self.layers[i+1]\n",
    "            activation = self.final_activation if i==len(self.layers)-2 else self.activation\n",
    "        else:\n",
    "            input_dim, output_dim = self.layers[i+1], self.layers[i]\n",
    "            activation = self.activation\n",
    "        model.add(Dense(output_dim=output_dim,\n",
    "                        input_dim=input_dim,\n",
    "                        init=self.init))\n",
    "        model.add(Activation(activation))\n",
    "        \n",
    "    def build(self):\n",
    "        self.encoder = Sequential()\n",
    "        self.decoder = Sequential()\n",
    "        self.autoencoder = Sequential()\n",
    "        for i in range(len(self.layers)-1):\n",
    "            self._add_layer(self.encoder, i, True)\n",
    "            self._add_layer(self.autoencoder, i, True)\n",
    "            #if i<len(self.layers)-2:\n",
    "            #    self.autoencoder.add(Dropout(self.dropout))\n",
    "\n",
    "        # Note that the decoder layers are in reverse order\n",
    "        for i in reversed(range(len(self.layers)-1)):\n",
    "            self._add_layer(self.decoder, i, False)\n",
    "            self._add_layer(self.autoencoder, i, False)\n",
    "            \n",
    "    def compile(self):\n",
    "        print(\"Compiling the encoder ...\")\n",
    "        self.encoder.compile(loss='categorical_crossentropy', optimizer=self.optimizer, metrics=self.metrics)\n",
    "        print(\"Compiling the decoder ...\")\n",
    "        self.decoder.compile(loss='mse', optimizer=self.optimizer, metrics=self.metrics)\n",
    "        print(\"Compiling the autoencoder ...\")\n",
    "        return self.autoencoder.compile(loss='mse', optimizer=self.optimizer, metrics=self.metrics)\n",
    "    \n",
    "    def fit(self, X_train, Y_train, batch_size, nb_epoch, verbose=1):\n",
    "        result = self.autoencoder.fit(X_train, Y_train,\n",
    "                                      batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                                      verbose=verbose)\n",
    "        # copy the weights to the encoder\n",
    "        for i, l in enumerate(self.encoder.layers):\n",
    "            l.set_weights(self.autoencoder.layers[i].get_weights())\n",
    "        for i in range(len(self.decoder.layers)):\n",
    "            self.decoder.layers[-1-i].set_weights(self.autoencoder.layers[-1-i].get_weights())\n",
    "        return result\n",
    "    \n",
    "    def pretrain(self, X_train, batch_size, nb_epoch, verbose=1):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            # Greedily train each layer\n",
    "            print(\"Now pretraining layer {} [{}-->{}]\".format(i+1, self.layers[i], self.layers[i+1]))\n",
    "            ae = Sequential()\n",
    "            self._add_layer(ae, i, True)\n",
    "            #ae.add(Dropout(self.dropout))\n",
    "            self._add_layer(ae, i, False)\n",
    "            ae.compile(loss='mse', optimizer=self.optimizer, metrics=self.metrics)\n",
    "            ae.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose)\n",
    "            # Then lift the training data up one layer\n",
    "            print(\"\\nTransforming data from\", X_train.shape, \"to\", (X_train.shape[0], self.layers[i+1]))\n",
    "            enc = Sequential()\n",
    "            self._add_layer(enc, i, True)\n",
    "            enc.compile(loss='mse', optimizer=self.optimizer, metrics=self.metrics)\n",
    "            enc.layers[0].set_weights(ae.layers[0].get_weights())\n",
    "            enc.layers[1].set_weights(ae.layers[1].get_weights())\n",
    "            X_train = enc.predict(X_train, verbose=verbose)\n",
    "            print(\"\\nShape check:\", X_train.shape, \"\\n\")\n",
    "            # Then copy the learned weights\n",
    "            self.encoder.layers[2*i].set_weights(ae.layers[0].get_weights())\n",
    "            self.encoder.layers[2*i+1].set_weights(ae.layers[1].get_weights())\n",
    "            self.autoencoder.layers[2*i].set_weights(ae.layers[0].get_weights())\n",
    "            self.autoencoder.layers[2*i+1].set_weights(ae.layers[1].get_weights())\n",
    "            self.decoder.layers[-1-(2*i)].set_weights(ae.layers[-1].get_weights())\n",
    "            self.decoder.layers[-1-(2*i+1)].set_weights(ae.layers[-2].get_weights())\n",
    "            self.autoencoder.layers[-1-(2*i)].set_weights(ae.layers[-1].get_weights())\n",
    "            self.autoencoder.layers[-1-(2*i+1)].set_weights(ae.layers[-2].get_weights())\n",
    "            \n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        return self.autoencoder.evaluate(X_test, Y_test)\n",
    "    \n",
    "    def predict(self, X, verbose=False):\n",
    "        return self.autoencoder.predict(X, verbose=verbose)\n",
    "\n",
    "    def _get_paths(self, name):\n",
    "        model_path = \"models/{}_model.yaml\".format(name)\n",
    "        weights_path = \"models/{}_weights.hdf5\".format(name)\n",
    "        return model_path, weights_path\n",
    "\n",
    "    def save(self, name='autoencoder'):\n",
    "        model_path, weights_path = self._get_paths(name)\n",
    "        open(model_path, 'w').write(self.autoencoder.to_yaml())\n",
    "        self.autoencoder.save_weights(weights_path, overwrite=True)\n",
    "    \n",
    "    def load(self, name='autoencoder'):\n",
    "        model_path, weights_path = self._get_paths(name)\n",
    "        self.autoencoder = keras.models.model_from_yaml(open(model_path))\n",
    "        self.autoencoder.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nb_epoch = 3\n",
    "sae = StackedAutoencoder(layers=[nb_input, 500, 150, 50, 10],\n",
    "                         activation='sigmoid',\n",
    "                         final_activation='softmax',\n",
    "                         init='uniform',\n",
    "                         dropout=0.25,\n",
    "                         optimizer='SGD')   # replace with 'adam', 'relu', 'glorot_uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sae.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_10_by_20_images(images_test, to_images(sae.predict(X_test)), figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_sae_prediction(j):\n",
    "    X_plot = X_test[j:j+1]\n",
    "    prediction = sae.predict(X_plot, verbose=False)\n",
    "    plot_1_by_2_images(to_images(X_plot)[0], to_images(prediction)[0])\n",
    "    print(sae.encoder.predict(X_plot, verbose=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(draw_sae_prediction, j=(0, len(X_test)-1))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sae.pretrain(X_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Visualising the Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def visualise_filter(model, layer_index, filter_index):\n",
    "    from keras import backend as K\n",
    "\n",
    "    # build a loss function that maximizes the activation\n",
    "    # of the nth filter on the layer considered\n",
    "    layer_output = model.layers[layer_index].get_output()\n",
    "    loss = K.mean(layer_output[:, filter_index])\n",
    "\n",
    "    # compute the gradient of the input picture wrt this loss\n",
    "    input_img = model.layers[0].input\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # we start from a gray image with some noise\n",
    "    input_img_data = np.random.random((1,nb_input,))\n",
    "    # run gradient ascent for 20 steps\n",
    "    step = 1\n",
    "    for i in range(100):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        #print(\"Current loss value:\", loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "    print(\"Current loss value:\", loss_value)\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value>0:\n",
    "        #return input_img_data[0]\n",
    "        return input_img_data\n",
    "    else:\n",
    "        raise ValueError(loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_filter(i):\n",
    "    flt = visualise_filter(mlp, 3, 4)\n",
    "    #print(flt)\n",
    "    plot_mnist_digit(to_images(flt)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "interact(draw_filter, i=[0, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We're hiring! ([bitbucket.org/argonasset/opportunities](bitbucket.org/argonasset/opportunities))\n",
    "\n",
    "<img src='img/argon_website.png' align='center' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you\n",
    "\n",
    "  * Come join the [Cape Town Deep Learrning Meetup](http://www.meetup.com/Cape-Town-deep-learning/)!\n",
    "  * Get the code: [github.com/snth/ctdeep](github.com/snth/ctdeep)\n",
    "  * We're hiring --> [bitbucket.org/argonasset/opportunities](bitbucket.org/argonasset/opportunities)\n",
    "  * Email me: [tobias@argonasset.co.za](mailto:tobias@argonasset.co.za)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "3f01ccac0e5143e791db92fbbb5fa81a": {
     "views": [
      {
       "cell_index": 90
      }
     ]
    },
    "4778531263214789810fe1539ba342ea": {
     "views": [
      {
       "cell_index": 84
      }
     ]
    },
    "8748a02324eb48f3a8badd0bb8ba520f": {
     "views": [
      {
       "cell_index": 59
      }
     ]
    },
    "8b143b97ccab4bf2817add62d0fe08f9": {
     "views": [
      {
       "cell_index": 102
      }
     ]
    },
    "a0a9da45bf554b9fa6798bea78c53ef4": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    },
    "e434e8420ec44dc48bbb1161dd992738": {
     "views": [
      {
       "cell_index": 98
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
